---
title: MicroNet for Efficient Language Modeling
booktitle: Proceedings of the NeurIPS 2019 Competition and Demonstration Track
year: '2020'
abstract: It is important to design compact language models for efficient deployment.
  We improve upon recent advances in both the language modeling domain and the model-compression
  domain to construct parameter and computation efficient language models. We use
  an efficient transformer-based architecture with adaptive embedding and softmax,
  differentiable non-parametric cache, Hebbian softmax, knowledge distillation, network
  pruning, and low-bit quantization. In this paper, we provide the winning solution
  to the NeurIPS 2019 MicroNet Challenge in the language modeling track. Compared
  to the baseline language model provided by the MicroNet Challenge, our model is
  90 times more parameter-efficient and 36 times more computation-efficient while
  achieving the required test perplexity of 35 on the Wikitext-103 dataset. We hope
  that this work will aid future research into efficient language models, and we have
  released our full source code at {https://github.com/mit-han-lab/neurips-micronet}.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yan20a
month: 0
tex_title: MicroNet for Efficient Language Modeling
firstpage: 215
lastpage: 231
page: 215-231
order: 215
cycles: false
bibtex_author: Yan, Zhongxia and Wang, Hanrui and Guo, Demi and Han, Song
author:
- given: Zhongxia
  family: Yan
- given: Hanrui
  family: Wang
- given: Demi
  family: Guo
- given: Song
  family: Han
date: 2020-08-19
address: 
container-title: Proceedings of the NeurIPS 2019 Competition and Demonstration Track
volume: '123'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 19
pdf: http://proceedings.mlr.press/v123/yan20a/yan20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
